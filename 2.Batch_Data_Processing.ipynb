{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Batch Data Processing - Batch processing of  Electronic Medical Reports (EMR) with Amazon Comprehend Medical\n",
    "\n",
    "In the previous module, [1.Data_Processing](./1.Data_processing.ipynb), we learnt how to extract medical information from a single PDF medical report using Textract and Comprehend Medical. In this step, we will be preparing our dataset for the classification machine learning model that we will be building. We will use the same pre-processing methodology from the previous step to process a batch of medical reports. But to minimise processing time and cost for this lab, we will skipping the Textract step and focusing on be processing a batch of medical reports in textual format using Comprehend Medical. We use this output from Comprehend Medical as our dataset to train our machine learning model.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Objective](#Objective)\n",
    "1. [Background](#Background)\n",
    "1. [Setup Environment](#Setup-Environment)\n",
    "1. [Load and Explore Data](#Load-and-Explore-Data)\n",
    "1. [Data Sampling for modeling](#Data-Sampling-for-modeling)\n",
    "1. [Combine the dataset](#Combine-the-dataset)\n",
    "1. [Save the processed file](#Save-the-processed-file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objective \n",
    "This notebook is the preprocessing step to prepare a batch of medical records for model training. Specifically, you will use Comprehend Medical to extract medical terms from doctors's transcripts and organize them into data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "**Dataset**: Medical transcription data scraped from mtsamples.com. The target in the preprocess is to extract the medical conditions from doctors' notes, and then organized into dataset for modeling. In the next step, we will use the processed dataset to correctly classify the medical specialties based on the transcription text. In real life, the model can be used for automatic reference to respective specialist.\n",
    "\n",
    "**Amazon Comprehend Medical**: Comprehend Medical detects useful information in unstructured clinical text. As much as 75% of all health record data is found in unstructured text such as physician's notes, discharge summaries, test results, and case notes. Amazon Comprehend Medical uses Natural Language Processing (NLP) models to sort through text for valuable information.\n",
    "\n",
    "**Supported Languages**: Amazon Comprehend Medical only detects medical entities in English language texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup Environment\n",
    "\n",
    "Before we begin, let us setup our environment, will be doing the following:\n",
    "\n",
    "- **import** some useful libraries (as in any Python notebook)\n",
    "- **configure** the S3 bucket and folder where data should be stored (to keep our environment tidy)\n",
    "- **connect** to Amazon Comprehend(with [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)) and SageMaker in particular (with the [sagemaker SDK](https://sagemaker.readthedocs.io/en/stable/)), to use the cloud services\n",
    "\n",
    "Note: While `boto3` is the general AWS SDK for Python, `sagemaker` provides some powerful, higher-level interfaces designed specifically for ML workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # For matrix operations and numerical processing\n",
    "import pandas as pd  # For munging tabular data\n",
    "import time\n",
    "import os\n",
    "from util.preprocess import *  # helper function for classification reports\n",
    "\n",
    "# setting up SageMaker parameters\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "bucket_prefix = \"emr-mtSample\"  # Location in the bucket to store our files\n",
    "sgmk_session = sagemaker.Session()\n",
    "\n",
    "sgmk_client = boto_session.client(\"sagemaker\")  ## API for sagemaker\n",
    "cm_client = boto3.client(service_name='comprehendmedical', use_ssl=True, region_name = 'us-east-1') ## API for comprehend medical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load and Explore Data\n",
    "\n",
    "For this workshop, we have already included the MTSamples dataset in your local folder (`./data/mtsample.csv`). You can find the raw dataset is available at [kaggle](https://www.kaggle.com/tboyle10/medicaltranscriptions). \n",
    "\n",
    "**Columns in the dataset**:\n",
    "\n",
    "* `description`: Short description of transcription (string)\n",
    "* `medical_specialty`: Medical specialty classification of transcription (string)\n",
    "* `sample_name`: Transcription title\n",
    "* `transcription`: Transcribed doctors' notes\n",
    "* `keywords`: Relevant keywords from transcription\n",
    "\n",
    "In the next section of our workshop, [3.Model building, training and deployment](./3.Model_building_training_and_development.ipynb), we will be building a machine learning model to classify medical reports based on the transcription. But before we can do that, we need to extract data from the MTSamples dataset. To train our model, we will be extracting the medical information from  the `transcription` column using Amazon Comprehend Medical. We will also be using the `medical_specialty`column as our labels for our classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"./data/mtsamples.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up dataset\n",
    "\n",
    "Before we explore and process the dataset, let us check for empty columns and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum(axis=0) ## check for missing information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the *33* rows with `transcription` is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['transcription'].isnull()==False].reset_index()\n",
    "df.isnull().sum(axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore dataset by medical speciality\n",
    "\n",
    "Let us do some data exploration and observe the distribution of medical reports by medical speciality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add patient_id for reference\n",
    "df['id']=df.index\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "sns.countplot(y='medical_specialty',order=df['medical_specialty'].value_counts().index, data=df)  #df.medical_specialty.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Sampling for modeling\n",
    "\n",
    "#### Business Question to be addressed here:\n",
    "Based on the medical description of this patient, should he/she go just for *Consulation* or be referred to *Surgery* specialist. \n",
    "\n",
    "#### ML problem to resolve:\n",
    "Binary classfication-*Consulation* or *Surgery* - for patient based on medical conditions.\n",
    "\n",
    "#### Why we do data sampling at this step?  \n",
    "\n",
    "For demo purpose and time concern, we will sample 200 records from both categories (in total, we have 1000+ medical reports that have been classified under the **Surgery**  and 400+ under the **Consult** medical speciality) to conduct binary classification for step 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sampling 1- Sampling from Surgery\n",
    "\n",
    "Let's randomly select 200 patients from the **Surgery** category and extract the medical conditions using Amazon Comprehend Medical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nSample=20 ## <specify a number to process the medical terms in a batch>. We recommend 20 for time consideration\n",
    "medical_specialty=' Surgery'\n",
    "df_phyList1,patient_ids=subpopulation_comprehend(df, medical_specialty,sampleSize=nSample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Do a batch processing using Amazon Comprehend Medical \n",
    "\n",
    "In this challenge, you are required to write your code within for-loop. You are expected to extract all the medical_conditions for each patient, together with the confidence score \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to process a multiple records\n",
    "def extractMCbatch(transcriptionList,patientIDList):\n",
    "    df_final = pd.DataFrame()\n",
    "    \n",
    "    if(len(transcriptionList)!=len(patientIDList)):\n",
    "        return(\"Error! different length!\")\n",
    "    \n",
    "    ## In this for loop, gererate a wide dataframe with extracted medical condition from each item, together with the corresponding ID \n",
    "    for item,patient_id in zip(transcriptionList,patientIDList):\n",
    "        print(\"processing patient_id:\",patient_id )\n",
    "        \n",
    "        ###########################################################################\n",
    "        ## TODO: WRITE YOUR CODE HERE TO GENERATE A ROW FOR THE ITEM FOR THE PATIENT!\n",
    "        ###########################################################################\n",
    "    \n",
    "    # remove the duplicated entries if any\n",
    "    df_final=df_final.sort_values(by=['ID','MEDICAL_CONDITION']).drop_duplicates(['ID','MEDICAL_CONDITION'],keep='last')\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stucked?\n",
    "Feel free to check the hint/recording section in [*step 4*](https://www.aiml-loft.wwps.aws.dev/workshops/module-medical-document-processing-and-classification/step2/) on the website "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your function *extractMCbatch* and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extracted_surg=extractMCbatch(df_phyList1,patient_ids)\n",
    "\n",
    "## plot the results\n",
    "topN=20 ## the number for top conditions\n",
    "threshold_score=0.9 ##the threshold of confidence score\n",
    "df_surg_plot=mc_barplot(df_extracted_surg, threshold_score,topN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sampling 2 - *Consult - History and Phy*\n",
    "\n",
    "Let us pick another medical speciality we want to try and classify. Let us pick 200 medical reports from the next most popular medical speciality - *Consult - History and Phy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nSample=20 ##<specify a number to process the medical terms in a batch >\n",
    "medical_specialty=' Consult - History and Phy.'\n",
    "df_phyList,patient_ids=subpopulation_comprehend(df, medical_specialty,sampleSize=nSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## organize into a dataframe\n",
    "df_extracted_consult=extractMCbatch(df_phyList,patient_ids)\n",
    "## plot the results\n",
    "topN=20 ## the number for top conditions\n",
    "threshold_score=0.9\n",
    "mc_barplot(df_extracted_consult, threshold_score,topN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Combine the dataset\n",
    "\n",
    "Until now, you have successfully generated 2 dataframes! One is for referal to *`Surgery`* specialist and the other for just for *`Consultation`*. \n",
    "\n",
    "After sampling the 2 medical specialities for classification, let's consolidate this data and save it for the next part of the workshop.\n",
    "\n",
    "### Gaps:\n",
    "After taking a closer look at the dataset, you will observe that the dataset is in long format, meaning that each row represents a single medical condition for one patients. If a patient *John* has 10 medical conditions, there will be 10 rows. Thus, you may expect varied number of rows of each patient. \n",
    "### Solutions:\n",
    "To make the dataset easier for ML algorithm to handle, we need to convert them into wide format, one row for one patient. Instead of keeping all the existing medical conditions, let's select the top `20` medical conditions from each category as input features. Note that `20` here is an arbitrary number which you can increase or decrease based on your experience.\n",
    "\n",
    "In the following cells, function *`retrieve_mcList(df, nFeature=20,threshold=0.9)`* helps to retrieve the features from each subset with `nFeature`(default=20)  as specified number of features and `threshold`(default=0.9) as the confidence threshold. Outputs from *`retrieve_mcList()`*:\n",
    "\n",
    "+ top medical conditions list,\n",
    "+ cleaned dataframe through converting to lower case, merg *etc*.\n",
    "\n",
    "\n",
    "`Target column`: as it is a binary classification problem, we defined a new column called `Label`, representing whether the patient need to go through surgery (`True`) or not (`False`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extracted_surg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extracted_consult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcList1, df_grp1=retrieve_mcList(df_extracted_surg, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcList2, df_grp2=retrieve_mcList(df_extracted_consult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Label the target column \"Label\": True--> go to see Surgery specialist, False --> only Consultation\n",
    "df_grp1['Label']=True # group one is Labeled as True \n",
    "df_grp2['Label']=False  # group two is Labeled as False \n",
    "\n",
    "\n",
    "df_combined=df_grp1.append(df_grp2) ## append two data frames \n",
    "mcLists=list(set(mcList1+mcList2))\n",
    "\n",
    "df_combined2=df_mc_generator(df_combined,mcLists ,colname_other=['ID',\"Label\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save the processed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined2.to_csv(\"./data/processed_combined_wide_short.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
