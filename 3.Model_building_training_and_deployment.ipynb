{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building an XGBoost model to classify electronic medical records (EMR) using Sagemaker\n",
    "\n",
    "In the previous module of the workshop, we prepared our dataset by extracting medical records that had a medical speciality of *`Surgery`* and *`Consultation`*. In this module, we will be using this dataset to create a classification model that will look at the transcription and categorize them into the following two categories - undergoing *`Consultation`* only or refer to a specialist for *`Sugery`*.\n",
    "\n",
    "The goal of this experiment is to do a **Next step Prediction** which aims at predicting the speciality needed for a patient with certain diseases. In practice, the model could be used to analyze a medical transcription in real-time and provide a recommended referals to respective specialist. \n",
    "\n",
    "The input is the EMR as a raw text file with doctor's notes about the patient, including his/her age, compaints described in free way, patient's history and so on. It is unstructured - different sections of oen patient anamnesis may abscent in another's.\n",
    "\n",
    "The value on the solution might be found in helping a doctor to find the optimal solution for diasnostics order. Patient can save time and money, and doctor can serve a patient more efficiently on sparing time for unnecessary diagnostics. Moreover, in difficult cases the algorithm may help a doctor to find a diagnosys faster, which in some cases may be extremely valuable, up to saving lives.\n",
    "\n",
    "Theoretically some regularities found by the algorithm may help medical researchers to find the idea of treating some deseases, based on their unobvious interconnections with some symptoms.\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Objective](#Objective)\n",
    "1. [Setup Environment](#Setup-Environment)\n",
    "1. [Load and Explore the Dataset](#Load-and-Explore-Dataset)\n",
    "1. [Prepare Dataset for Model Training](#Prepare-Dataset-for-Model-Training)\n",
    "1. [Understand the Algorithm](#Understand-the-Algorithm)\n",
    "1. [Train the Model](#Train-the-Model)\n",
    "1. [Deploy and Evaluate the Model](#Deploy-and-Evaluate-the-Model)\n",
    "1. [Challenges](#Challenges)\n",
    "1. [Hyperparameter Optimization](#Hyperparameter-Optimization)\n",
    "1. [Bonus Activtity](#Bonus-Activtity)\n",
    "1. [Conclusion](#Conclusion)\n",
    "1. [Clean up resources](#Clean-up-resources)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Objective \n",
    "\n",
    "In this section of the workshop we will walkthrough an example of using Sagemaker to train a machine learning model to classify medical documents using a SageMaker's built-in algorithm- XGboost. \n",
    " \n",
    "**Bonus activity**: The workshop will also include a bonus activity whereby you will learn how to use SageMaker's automatic Hyperparameter Optimization (HPO) capabilities to improve the model's performance with needing to tune the hyperparameters manually.\n",
    "\n",
    "**Note**: Teaching in-depth data science approaches for tabular data is outside this scope, however we recommend that you use this notebook as a reference to adapt to your needs for future projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup Environment\n",
    "\n",
    "Before we can begin, we will need to setup up notebook environment by performing some of the following environment setup:\n",
    "\n",
    "- **import** some useful libraries (as in any Python notebook)\n",
    "- **configure** the S3 bucket and folder where data should be stored (to keep our environment tidy)\n",
    "- **connect** to AWS in general (with [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)) and SageMaker in particular (with the [sagemaker SDK](https://sagemaker.readthedocs.io/en/stable/)), to use the cloud services\n",
    "- **Upgrad** SageMaker to the latest version\n",
    "\n",
    "While `boto3` is the general AWS SDK for Python, `sagemaker` provides some powerful, higher-level interfaces designed specifically for ML workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # For matrix operations and numerical processing\n",
    "import pandas as pd  # For munging tabular data\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "# import self-defined functions\n",
    "from util.classification_report import generate_classification_report, predict_from_numpy_V2  # helper function for classification reports\n",
    "from util.Pipeline import extractTextract, extractMedical\n",
    "from util.preprocess import *\n",
    "\n",
    "# setting up SageMaker parameters\n",
    "import pkg_resources\n",
    "pkg_resources.require(\"sagemaker>2.9.2\") \n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "bucket_prefix = \"emr-mtSample\"  # Location in the bucket to store our files\n",
    "sgmk_session = sagemaker.Session()\n",
    "sgmk_client = boto_session.client(\"sagemaker\")\n",
    "sgmk_role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load and Explore Dataset\n",
    "\n",
    "To begin, we will load the dataset from our previous notebook [2.Batch_Data_Processing](./2.Batch_Data_Processing.ipynb). This dataset contains labelled data based on the medical speciality - Surgery or not Surgery and the medical conditions that were extracted from the electronic medical reports.\n",
    "You can find the processed dataset in the following location '/data/processed_combined_wide.csv'.\n",
    "\n",
    "**Note**: The original raw dataset that this workshop is based on is available at [kaggle](https://www.kaggle.com/tboyle10/medicaltranscriptions).\n",
    "\n",
    "*Demographics:*\n",
    "* `ID`: id of the patients (int)\n",
    "* `Label`: the patient needs surgery? (bool)\n",
    "* `nontender`: medical condition extracted from doctos'notes. the number indicate confidence of the symptom (float)\n",
    "* the rest columns are other medical condistions ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_wide=pd.read_csv(\"./data/processed_combined_wide.csv\")\n",
    "df_wide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore correlation between the input variables and output one\n",
    "\n",
    "Let us start by looking at the correlation between the input features and our label (Surgry/Not Surgery).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrPlot(df_wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "+ Observation 1: *`wound`* and *`hypertension`* is postively and negatively correlated with Surgery. \n",
    "+ Observation 2: *`nausea` and *`vomitting`* is positively correlated\n",
    "+ Did you observe any more from the plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare-Dataset-for-Model-Training\n",
    "\n",
    "Next, we will start training the model. But before we can proceeed, we need to:\n",
    "\n",
    "1. Suffle and split the data into **Training (80%)**, **Validation (10%)**, and **Test (10%)** sets\n",
    "2. Convert the data to the format the algorithm expects (e.g. CSV)\n",
    "3. Upload the data to S3\n",
    "4. Create `s3_input` objects defining the data sources for the SageMaker SDK\n",
    "\n",
    "The training and validation datasets will be used during the training (and tuning) phase, while the 'holdout' test set will be used afterwards to evaluate the model.\n",
    "\n",
    "Please note that to train the SageMaker XGBoost algorithm, it expects data in the **libSVM** or **CSV** formats with the following format:\n",
    "\n",
    "- The target variable in the first column, and\n",
    "- No header row\n",
    "\n",
    "You can find more information about this [here](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html#InputOutput-XGBoost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the id column \n",
    "df_combined_model=df_wide.iloc[:,1:] \n",
    "# Shuffle and splitting dataset\n",
    "train_data, validation_data, test_data = np.split(df_combined_model.sample(frac=1, random_state=123), \n",
    "                                                  [int(0.8 * len(df_combined_model)), int(0.9*len(df_combined_model))],) \n",
    "\n",
    "# Create CSV files for Train / Validation / Test\n",
    "train_data.to_csv(\"data/train.csv\", index=False, header=False)\n",
    "validation_data.to_csv(\"data/validation.csv\", index=False, header=False)\n",
    "test_data.to_csv(\"data/test.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload CSV files to S3 for SageMaker training\n",
    "train_uri = sgmk_session.upload_data(\n",
    "    path=\"data/train.csv\",\n",
    "    bucket=bucket_name,\n",
    "    key_prefix=bucket_prefix\n",
    ")\n",
    "val_uri = sgmk_session.upload_data(\n",
    "    path=\"data/validation.csv\",\n",
    "    bucket=bucket_name,\n",
    "    key_prefix=bucket_prefix\n",
    ")\n",
    "\n",
    "\n",
    "# Create s3_inputs\n",
    "\n",
    "s3_input_train = sagemaker.TrainingInput(s3_data=train_uri, content_type=\"csv\")\n",
    "s3_input_validation = sagemaker.TrainingInput(s3_data=val_uri, content_type=\"csv\")\n",
    "\n",
    "print(f\"{s3_input_train.config}\\n\\n{s3_input_validation.config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand the Algorithm\n",
    "**`XGBoost`** stands for e**X**treme **G**radient **Boosting**. It implements the gradient boosting decision tree algorithm, which is an approach where new models are created that predict the residuals or errors of prior models and then added together to make the final prediction. It is called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models. Check detailed documentation for built-in *XGBoost* [here](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html).\n",
    "\n",
    "*\"The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms. Which is the reason why many people use xgboost.\"* -- Tianqi Chen, Creator of *XGBoost*\n",
    "\n",
    "The two major advantages of using XGBoost are:\n",
    "\n",
    "    1. Fast Execution Speed: Generally, XGBoost is faster when compared to other implementations of gradient boosting.\n",
    "    2. High Model Performance: XGBoost has exceled in either structured or tabular datasets on classification and regression predictive modeling problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris \n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "training_image = retrieve(\"xgboost\",region=region,  version=\"1.0-1\")\n",
    "\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters & Algorithm\n",
    "In the following step, we will use the [sagemaker.estimator.Estimator()](https://sagemaker.readthedocs.io/en/v1.72.0/api/training/estimators.html) function to configure the following:\n",
    "\n",
    "* image_name - Training image to use(image_name), in this case we will be using the xgboost training image\n",
    "* train_instance_type - Type of instance to use.\n",
    "* train_instance_count - The number of instances to run the training job. For suitable algorithms that support distributed training, set an instance count of more than 1.\n",
    "* role - IAM role used to run the training job\n",
    "* train_use_spot_instances - Specify whether to use spot instances. For more information about spot training, refer to the following url: https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html\n",
    "* train_max_run - Timeout in seconds for training (default: 24 * 60 * 60). After this amount of time Amazon SageMaker terminates the job regardless of its current status.\n",
    "* train_max_wait - Timeout in seconds waiting for spot training instances\n",
    "* hyperparameters - Our hyperparameters used to train the model\n",
    "\n",
    "For more information about the available xgboost hyperparamters, please refer to the following documentation [here](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"num_round\": \"150\",     # int: [1,300]\n",
    "    \"max_depth\": \"5\",     # int: [1,10]\n",
    "    \"alpha\": \"2.5\",         # float: [0,5]\n",
    "    \"eta\": \"0.5\",           # float: [0,1]\n",
    "    \"objective\": \"binary:logistic\",\n",
    "}\n",
    "\n",
    "# Instantiate an XGBoost estimator object\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=training_image,           # XGBoost algorithm container\n",
    "    instance_type=\"ml.m5.xlarge\",  # type of training instance\n",
    "    instance_count=1,              # number of instances to be used\n",
    "    role=sgmk_role,                      # IAM role to be used\n",
    "    use_spot_instances=True,       # Use spot instances to reduce cost\n",
    "    max_run=20*60,                 # Maximum allowed active runtime\n",
    "    max_wait=30*60,                # Maximum clock time (including spot delays)\n",
    "    hyperparameters=hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train-the-Model\n",
    "\n",
    "To start the training job, we will call the `estimator.fit()` function. This will start a Sagemaker training job in the background. You can also see your training job within the AWS console by going to Sagemaker -> Training jobs.\n",
    "\n",
    "Once the training job is completed, proceed to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start a training (fitting) job\n",
    "estimator.fit({ \"train\": s3_input_train, \"validation\": s3_input_validation })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy and Evaluate the Model\n",
    "Now that we've trained our xgboost model, let us proceed with deploying our model (hosting it behind a real-time endpoint) so that we can start running predictions in real-time. This can be done using the `estimator.deploy()` function. You can find more information about model deployment here - https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html.\n",
    "\n",
    "For the input to the deployment function, we will specify the following:\n",
    "\n",
    "* initial_instance_count - Minimum number of EC2 instances to deploy to an endpoint for prediction.\n",
    "* instance_type - Type of EC2 instance to deploy to an endpoint for prediction, for example, ‘ml.c4.xlarge’.\n",
    "\n",
    "This deployment might take up to 8 minutes, and by default the code will wait for the deployment to complete.\n",
    "If you like, you can instead:\n",
    "+ Un-comment the wait=False parameter\n",
    "+ Use the Endpoints page of the SageMaker Console to check the status of the deployment\n",
    "+ Skip over the Evaluation section below (which won't run until the deployment is complete), and start the Hyperparameter Optimization job - which will take a while to run too, so can be started in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(\n",
    "    #endpoint_name=auto_ml_job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    #inference_response_keys=inference_response_keys,\n",
    "    predictor_cls=sagemaker.predictor.Predictor,\n",
    "    #serializer = sagemaker.serializers.CSVSerializer()\n",
    "    #wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run predictions\n",
    "\n",
    "Once the Sagemaker endpoint has been deployed, we can now run some prediction to test our endpoint. Let us test our endpoint by running some predictions on our test data and evaluating the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (This cell will error, until the endpoint deployment above is finished!)\n",
    "\n",
    "# Get predictions for the test set:\n",
    "predictions = predict_from_numpy_V2(predictor, test_data.drop([\"Label\"], axis=1))\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "from util import classification_report\n",
    "reload(classification_report)\n",
    "\n",
    "predictions = predict_from_numpy_V2(predictor, test_data.drop([\"Label\"], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# helper function for classification reports\n",
    "\n",
    "generate_classification_report(\n",
    "    y_real=test_data['Label'].values, \n",
    "    y_predict_proba=predictions, \n",
    "    decision_threshold=0.5,\n",
    "    class_names_list=[\"Negative\", \"Positive\"],\n",
    "    title=\"Initial model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2a: Change the Hyperparameter \n",
    "Now that we've trained our model using some pre-defined hyperparamters, let us try and see if we can improve the model performance by mannually change the HyperParameter. For example, you can change the number of `num_round`, `max_depth` and the rest of the hyperparameter that we plan to tune.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!! TO DO: You are expected to fill the hyperparameters bellow and Run the Model training\n",
    "\n",
    "hyperparameters2 = {\n",
    "    \"num_round\": \"\",  ## TO WRITE   \n",
    "    \"max_depth\": \"\",  ## TO WRITE   \n",
    "    \"alpha\": \"\",      ## TO WRITE  \n",
    "    \"eta\": \"\",        ## TO WRITE   \n",
    "    \"objective\": \"binary:logistic\",\n",
    "}\n",
    "\n",
    "# Instantiate an XGBoost estimator object\n",
    "estimator2 = sagemaker.estimator.Estimator(\n",
    "    image_uri=training_image,           # XGBoost algorithm container\n",
    "    instance_type=\"ml.m5.xlarge\",  # type of training instance\n",
    "    instance_count=1,              # number of instances to be used\n",
    "    role=sgmk_role,                      # IAM role to be used\n",
    "    use_spot_instances=True,       # Use spot instances to reduce cost\n",
    "    max_run=20*60,                 # Maximum allowed active runtime\n",
    "    max_wait=30*60,                # Maximum clock time (including spot delays)\n",
    "    hyperparameters=hyperparameters2\n",
    ")\n",
    "estimator2.fit({ \"train\": s3_input_train, \"validation\": s3_input_validation }) ## model fitting \n",
    "estimator2.create_model() ## create a new model from the training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model from the training job\n",
    "\n",
    "After the training job is done, the model is not saved yet. Check training jobs and models in your SageMaker Console. To create a model from a training job, refer to the documentation for  *[create_model API](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!! TO DO: get the model artifact information\n",
    "\n",
    "## hint 1: create a primary container with the trained model\n",
    "## hint 2: check the API estimator2.create_model().model_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!! TO DO: create a model from the container\n",
    "\n",
    "## hint 3: estimator2.create_model().model_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stucked?\n",
    "Feel free to check the [video/solution in Step 3](https://www.aiml-loft.wwps.aws.dev/workshops/module-medical-document-processing-and-classification/step3/),  or reach out to any of us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2b: Update the endpoint \n",
    "\n",
    "Now, we've trained 2 models using pre-defined hyperparameter and deployed the first model. For the second model, instead of deploying a new model, can you update the endpoint with the newly built model? \n",
    "\n",
    "Hint 1: create a model with API *create_model*: https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-deploy-model.html \n",
    "\n",
    "The API that you are going to use *update_endpoint*: https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html#sagemaker.predictor.Predictor.update_endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "###########################################################################\n",
    "## !! TO DO: WRITE YOUR CODE HERE TO UPDATE THE END POINT ########################\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = predict_from_numpy(predictor, test_data.drop([\"Label\"], axis=1))\n",
    "predictions = predict_from_numpy_V2(predictor\n",
    "                                    , test_data.drop([\"Label\"], axis=1))\n",
    "generate_classification_report(\n",
    "    y_real=test_data['Label'].values, \n",
    "    y_predict_proba=predictions, \n",
    "    decision_threshold=0.5,\n",
    "    class_names_list=[\"Negative\", \"Positive\"],\n",
    "    title=\"updated model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Hyperparameter Optimization\n",
    "\n",
    "Now that we've trained our model using some pre-defined hyperparamters, let us try and see if we can improve the model performance by using SageMaker HyperParameter Optimization (HPO) by automating the search for an optimal hyperparameter. Specifically, we **specify a range**, or a list of possible values in the case of categorical hyperparameters, for each of the hyperparameter that we plan to tune.\n",
    "\n",
    "SageMaker hyperparameter tuning will automatically launch **multiple training jobs** with different hyperparameter settings, evaluate results of those training jobs based on a predefined \"objective metric\", and select the hyperparameter settings for future attempts based on previous results. For each hyperparameter tuning job, we will specify the maximum number of HPO tries (`max_jobs`) and how many of these can happen in parallel (`max_parallel_jobs`).\n",
    "\n",
    "Tip: `max_parallel_jobs` creates a **trade-off between performance and speed** (better hyperparameter values vs how long it takes to find these values). If `max_parallel_jobs` is large, then HPO is faster, but the discovered values may not be optimal. Smaller `max_parallel_jobs` will increase the chance of finding optimal values, but HPO will take more time to finish.\n",
    "\n",
    "Next we'll specify the objective metric that we'd like to tune and its definition, which includes the regular expression (Regex) needed to extract that metric from the CloudWatch logs of the training job. Since we are using built-in XGBoost algorithm here, it emits two predefined metrics: **validation:auc** and **train:auc**, and we elected to monitor *validation:auc* as you can see below. In this case (because it's pre-built for us), we only need to specify the metric name. For more information on parameter tuning of XGboost, please refer [here](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html) \n",
    "\n",
    "Depending on the number of tries, HPO can find a better performing model faster, compared to simply trying different hyperparameters by trial and error or grid search. You can learn more in-depth details about SageMaker HPO [here](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html).\n",
    "\n",
    "For more information on Sagemaker HPO please refer to the documentation [here](https://sagemaker.readthedocs.io/en/stable/tuner.html).\n",
    "\n",
    "**Note:** with the default settings below, the hyperparameter tuning job can take up to ~20 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "# set up hyperparameter ranges\n",
    "ranges = {\n",
    "    \"num_round\": IntegerParameter(100, 300),\n",
    "    \"max_depth\": IntegerParameter(1, 10),\n",
    "    \"alpha\": ContinuousParameter(0, 5),\n",
    "    \"eta\": ContinuousParameter(0, 1),\n",
    "}\n",
    "\n",
    "# set up the objective metric\n",
    "objective = \"validation:auc\"\n",
    "#objective = \"validation:accuracy\"\n",
    "# instantiate a HPO object\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=estimator,              # the SageMaker estimator object\n",
    "    hyperparameter_ranges=ranges,     # the range of hyperparameters\n",
    "    max_jobs=10,                      # total number of HPO jobs\n",
    "    max_parallel_jobs=2,              # how many HPO jobs can run in parallel\n",
    "    strategy=\"Bayesian\",              # the internal optimization strategy of HPO\n",
    "    objective_metric_name=objective,  # the objective metric to be used for HPO\n",
    "    objective_type=\"Maximize\",        # maximize or minimize the objective metric\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# start HPO\n",
    "tuner.fit({ \"train\": s3_input_train, \"validation\": s3_input_validation })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# wait, until HPO is finished\n",
    "hpo_state = \"InProgress\"\n",
    "\n",
    "while hpo_state == \"InProgress\":\n",
    "    hpo_state = sgmk_client.describe_hyper_parameter_tuning_job(\n",
    "                HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)[\"HyperParameterTuningJobStatus\"]\n",
    "    print(\"-\", end=\"\")\n",
    "    time.sleep(60)  # poll once every 1 min\n",
    "\n",
    "print(\"\\nHPO state:\", hpo_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# deploy the best model from HPO\n",
    "hpo_predictor = tuner.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\",predictor_cls=sagemaker.predictor.Predictor,\n",
    "    serializer = sagemaker.serializers.CSVSerializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hpo_predictor.deserializer=sagemaker.deserializers.CSVDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the predicted probabilities of the best model\n",
    "hpo_predictions = predict_from_numpy_V2(hpo_predictor, test_data.drop([\"Label\"], axis=1))\n",
    "print(hpo_predictions)\n",
    "\n",
    "# generate report for the best model\n",
    "generate_classification_report(\n",
    "    y_real=test_data[\"Label\"].values, \n",
    "    y_predict_proba=hpo_predictions, \n",
    "    decision_threshold=0.5,\n",
    "    class_names_list=[\"Consultation\",\"Surgery\"],\n",
    "    title=\"Best model (with HPO)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "##  Bonus Activtity\n",
    "\n",
    "### A simplified pipeline to process an Electronic Health Record\n",
    "Here, we will combine Textract, Comprehend Medical and SageMaker endpoint to process an electronic medical resport. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "from util.Pipeline import extractTextract, extractMedical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extract data from Textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDFprefix = 'sagemaker/medical_notes'\n",
    "fileName =  'sample_report_1.pdf'\n",
    "#fileUploadPath = os.path.join('./data', fileName)\n",
    "textractObjectName = os.path.join(PDFprefix, 'data', fileName)\n",
    "print(\"EHR file to be processed is at \", textractObjectName)\n",
    "\n",
    "doc=extractTextract(bucket_name,textractObjectName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Extract data from Comprehend Medical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehendResponse=extractMedical(doc)\n",
    "df_cm=extractMC_v2(comprehendResponse[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Organize the extracted json file into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mclist, df_cm2=retrieve_mcList(df_cm, nFeature=20,threshold=0.9)\n",
    "df_cm2=df_mc_generator_slim(df_cm2)\n",
    "df_cm2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Prediction with the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pred = predict_from_numpy_V2(predictor, df_cm2.drop([\"ID\"], axis=1))\n",
    "sample_pred_hpo = predict_from_numpy_V2(hpo_predictor, df_cm2.drop([\"ID\"], axis=1))\n",
    "print(f\"Predicted probability for sugery for the patient is : {round(sample_pred[0],2)} from model1,\\n \\t \\t \\t \\t \\t\\t      {round(sample_pred_hpo[0],2)} from model2 after HPO \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "SageMaker built-in algorithms are great for getting a first model fast, and combining them with SageMaker HPO can really boost their accuracy. As we mentioned here, the best way to success with a built-in algorithm is to **read the [algorithm's doc pages](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html) carefully** - to understand what data format and parameters it needs!\n",
    "\n",
    "In our run, we have used a built-in algorithm XGBoost to train a classification model based on doctors' transcriptions. The first model with self-defined HyperParameters showed AUC of ~0.945, and the optimized HPO model exhibited an AUC of ~0.951: fairly higher !\n",
    "\n",
    "\n",
    "After that, we demonstrated a simple pipeline to process an electronic patient's record with the endpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Clean up resources\n",
    "### Delete the endpoint and configuration if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "hpo_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the generated files S3 bucket files\n",
    "+ check your [S3 bucket](https://s3.console.aws.amazon.com/s3/home?region=ap-southeast-1) for the content information\n",
    "+ delete all the saved models and training jobs in the bucket, under folder */emr-mtSample*\n",
    "+ delete the pdf files in S3 bucket under folder */sagemaker/medical_notes*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete all the content in the emr-mtSample folder. Check S3 before deleting it\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "bucket.objects.filter(Prefix=bucket_prefix).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Delete all the content in the PDF folder \n",
    "\n",
    "bucket.objects.filter(Prefix=PDFprefix).delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practice:\n",
    " 1. Delete the bucket if this is the last lab of your workshop\n",
    " 2. Shut down your notebook instance if you are not planning to explore more labs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
